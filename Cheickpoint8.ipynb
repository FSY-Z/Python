{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW8VXiucWbxD"
      },
      "outputs": [],
      "source": [
        "!pip install requests_html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from requests_html import HTMLSession\n",
        "from bs4 import BeautifulSoup\n",
        "g = HTMLSession()\n",
        "url = \"https://en.wikipedia.org/wiki/Amazon_(company)\"\n",
        "\n",
        "def data(x):\n",
        "  gt = g.get(x)\n",
        "  soup = BeautifulSoup(gt.text,'html.parser')\n",
        "  return soup\n",
        "\n",
        "soup = data(url)\n",
        "print(soup)"
      ],
      "metadata": {
        "id": "cnMN7XspWsTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title(x):\n",
        "  page_title = soup.select_one(\"#firstHeading > span\")\n",
        "  return page_title.get_text()\n",
        "\n",
        "title(soup)"
      ],
      "metadata": {
        "id": "SaUHt7gJYgfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paragraphs():\n",
        "  for i in soup.find_all(\"p\"):\n",
        "    print(i.get_text())\n",
        "paragraphs()"
      ],
      "metadata": {
        "id": "bmXF3nYRZVxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def links():\n",
        "  links = {}\n",
        "  for link in soup.find_all(\"a\"):\n",
        "    url = link.get(\"href\", \"\")\n",
        "    if \"/wiki/\" in url:\n",
        "      links[link.text.strip()] = url\n",
        "  print(links,\"\\n\")\n",
        "links()"
      ],
      "metadata": {
        "id": "7D7RA0CEa35e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from requests_html import HTMLSession\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scraper(wiki):\n",
        "\n",
        "  def data(url):\n",
        "    gt = g.get(url)\n",
        "    soup = BeautifulSoup(gt.text, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "  soup = data(wiki)\n",
        "  def title():\n",
        "    page_title = soup.select_one(\"#firstHeading > span\")\n",
        "    return page_title.get_text()\n",
        "\n",
        "\n",
        "  def paragraphs():\n",
        "   for head in soup.find_all(\"p\"):\n",
        "     print(head.get_text(),\"\\n\")\n",
        "  \n",
        "  def links():\n",
        "    links = {}\n",
        "    for link in soup.find_all(\"a\"):\n",
        "      url = link.get(\"href\", \"\")\n",
        "      if \"/wiki/\" in url:\n",
        "        links[link.text.strip()] = url\n",
        "    print(links,\"\\n\")\n",
        "  \n",
        "  print(title(),paragraphs(),links())\n",
        "\n",
        "scraper(\"https://en.wikipedia.org/wiki/Egypt\")"
      ],
      "metadata": {
        "id": "35OFdWrNbZs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}